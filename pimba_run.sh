#Authors: Renato Oliveira. Gisele Nunes, Raíssa Oliveira
#version: 1.7
#Date: 02-03-2021

###    Copyright (C) 2021  Renato Oliveira
###
###    This program is free software: you can redistribute it and/or modify
###    it under the terms of the GNU General Public License as published by
###    the Free Software Foundation, either version 3 of the License, or
###    any later version.
###
###    This program is distributed in the hope that it will be useful,
###    but WITHOUT ANY WARRANTY; without even the implied warranty of
###    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
###    GNU General Public License for more details.
###
###    You should have received a copy of the GNU General Public License
###    along with this program.  If not, see <http://www.gnu.org/licenses/>.

###Contacts:
###    guilherme.oliveira@itv.org
###    renato.renison@gmail.com

#bin/sh
##usage: ./pimba_run.sh -i <input_reads> -o <output_dir> -w <approach> -s <otu_similarity> -a <assign_similarity> -c <coverage> -l <otu_length> -h <hits_per_subject> -g <marker_gene> -t <num_threads> -e <E-value> -d <databases.txt> -x <run_lulu>
#-i <input_reads> = FASTA file with reads output generated by pimba_prepare.sh
#-o <output_dir> = Directory where the results will be stored.
#-w <approach> = Analysis strategy to be used. It can be 'otu' or 'asv'. If 'otu', pimba uses vsearch. If 'asv', pimba uses swarm. Default: 'otu'
#-s <otu_similarity> = Percentage of similarity that will be used in the otu clustering. Default is 0.97
#-a <assign_similarity> = Percentage of similarity that will be used in the taxonomy assignment. Default is 0.9
#-c <coverage> = minimum converage for the alignment. Default is 0.9
#-l <otu_length> = Length to trim the reads. If 0, then no reads are trimmed.
#-h <hits_per_subject> = if 1, choose the best hit. If > 1, choose by majority. Default is 1
#-g <marker_gene> = Marker gene and Database of the analisys. It can be: (16S-SILVA, 16S-GREENGENES, 16S-RDP, 16S-NCBI, ITS-FUNGI-NCBI, ITS-FUNGI-UNITE, ITS-PLANTS-NCBI, COI-NCBI, COI-BOLD)
#-t <num_threads> = Number or threads to use in the blast step. Default is 1.
#-e <E-value> = Expected value used by blast. Dafault is 0.00001.
#-d <databases_file.txt> = File with the databases path. Default is /bio/pimba_metabarcoding/databases.txt
#-x <lulu> = if set as 'lulu', PIMBA will discard erroneous OTUs or ASVs with LULU. Default is not to use LULU.
#-m <its> = if set as 'its', PIMBA will run the ITSx tool to extract only the intergenic regions and discard ribosomal data.
#USAGE: /bio/pimba_metabarcoding/pimba_run.sh -i AllSamplesCOI_chip1234_good.fasta -o AllSamplesCOI_98clust90assign -s 0.98 -a 0.9 -c 0.9 -l 130 -h 1 -g COI-ALL -t 24 -e 0.1 -d databases.txt -x lulu

#source activate qiime1

SCRIPT_PATH=/bio/share_bio/utils/renato/QiimePipe
HITS_PER_SUBJECT=1
SIMILARITY=0.97
SIMILARITY_ASSIGN=0.9
SIMILARITY_INT=$(bc -l <<<"${SIMILARITY}*100")
SIMILARITY_INT=${SIMILARITY_INT%.*}
SIMILARITY_INT_ASG=$(bc -l <<<"${SIMILARITY_ASSIGN}*100")
SIMILARITY_INT_ASG=${SIMILARITY_INT_ASG%.*}
COVERAGE=0.9
COVERAGE_INT=$(bc -l <<<"${COVERAGE}*100")
COVERAGE_INT=${COVERAGE_INT%.*}
LENGTH=200
THREADS=1
EVALUE=0.00001
APPROACH=otu
TIMESTAMP=$(date +"%Y%m%d%H%M%S")
LULU=no
ITS=no
#DB_FILE=/bio/pimba_metabarcoding/databases.txt

while getopts "i:o:w:s:a:c:l:h:g:t:e:d:x:m:" opt; do
	case $opt in
		i) RAWDATA="$OPTARG"
		;;
		o) OUTPUT="$OPTARG"
		;;
		w) APPROACH="$OPTARG"
		;;
		s) SIMILARITY="$OPTARG"
		;;
		a) SIMILARITY_ASSIGN="$OPTARG"
		;;
		c) COVERAGE="$OPTARG"
		;;
		l) LENGTH="$OPTARG"
		;;
		h) HITS_PER_SUBJECT="$OPTARG"
		;;
		g) GENE="$OPTARG"
		;;
		t) THREADS="$OPTARG"
        ;;
        e) EVALUE="$OPTARG"
        ;;
        d) DB_FILE="$OPTARG"
        ;;
        x) LULU="$OPTARG"
        ;;
        m) ITS="$OPTARG"
        ;;
		\?) echo "Invalid option -$OPTARG" >&2
    	;;
	esac
done

SIMILARITY_INT=$(bc -l <<<"${SIMILARITY}*100")
SIMILARITY_INT=${SIMILARITY_INT%.*}

COVERAGE_INT=$(bc -l <<<"${COVERAGE}*100")
COVERAGE_INT=${COVERAGE_INT%.*}

SIMILARITY_INT_ASG=$(bc -l <<<"${SIMILARITY_ASSIGN}*100")
SIMILARITY_INT_ASG=${SIMILARITY_INT_ASG%.*}

source $DB_FILE

# SILVA_DB_16S=/bio/share_bio/databases/Silva_132_release/SILVA_132_QIIME_release
# GG_DB_16S=/bio/share_bio/databases/gg_13_5_otus
# RDP_DB_16S=/bio/share_bio/databases/RDP
# NCBI_DB=/bio/share_bio/databases/NCBI/blast_nt/nt
# NCBI_DB_EXP=/bio/share_bio/databases/NCBI/blast_nt
# ITS_UNITE_DB=/bio/share_bio/databases/its_fungi/sh_refs_qiime_ver8.2
# ITS_BIOBASE_DB=/bio/share_bio/databases/metabarcoding_db/ITS_plants/ITS_plants
# COI_BIOBASE_DB=/bio/share_bio/databases/metabarcoding_db/COI_ITV/coi_itv
# COI_BOLD_DB=/bio/share_bio/databases/metabarcoding_db/COI_BOLD/coi_bold
# COI_ALL_DB=/bio/share_bio/databases/metabarcoding_db/COI_all/coi_all

CURRENT_PATH=$(pwd)

REALPATH_RAW=$(realpath $RAWDATA)
DIR_NAME_RAW=$(dirname $REALPATH_RAW)
FILE_NAME_RAW=$(basename $RAWDATA)
cd $DIR_NAME_RAW
FULL_PATH_RAW=$(pwd)
cd $CURRENT_PATH

pathlist=$(echo $FULL_PATH_RAW; echo $CURRENT_PATH)
#COMMON_PATH=$(i=2; while [ $i -lt 500 ]; do   path=`echo "$pathlist" | cut -f1-$i -d/ | uniq -d`;   if [ -z "$path" ];   then      echo $prev_path;      break;   else      prev_path=$path;   fi;   i=`expr $i + 1`; done);

#COMMON_PATH=$({ echo $FULL_PATH_RAW; echo $CURRENT_PATH;} | sed -e 'N;s/^\(.*\).*\n\1.*$/\1\n\1/;D')

#echo Common Path: $COMMON_PATH
echo Current Path: $CURRENT_PATH

mkdir $OUTPUT
cd $OUTPUT

newfile="$(basename $RAWDATA .fasta)"

#Dereplication <<<USING USEARCH 7>>>
echo "Creating a VSEARCH Container: "
docker run -id -v $CURRENT_PATH:/output/ -v $FULL_PATH_RAW:/rawdata/ --name vsearch_run_$TIMESTAMP itvdsbioinfo/pimba_vsearch:v2.15.2


if [ $GENE = "ITS-FUNGI-NCBI" ] || [ $GENE = "ITS-FUNGI-UNITE" ];
then
	echo  "Creating an ITSx Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $FULL_PATH_RAW:/rawdata/ --name itsx_run_$TIMESTAMP metashot/itsx:1.1.2-1

	echo  "Running the ITSx Container: "
	docker exec -u $(id -u) -i itsx_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		ITSx -i /rawdata/'${FILE_NAME_RAW}' -o '${newfile}_itsx' -t f \
		--cpu '$THREADS' --graphical F; chmod -R 777 /output/'$OUTPUT';'

	cat *_itsx.ITS1.fasta *_itsx.ITS2.fasta > ${newfile}_itsx.fasta
	chmod 777 ${newfile}_itsx.fasta

	echo "Running the VSEARCH Container - --derep_fulllength: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --derep_fulllength '${newfile}_itsx.fasta' --output '${newfile}_derep.fasta' --sizeout; \
		chmod -R 777 /output/'$OUTPUT';'
	#vsearch --derep_fulllength ../${RAWDATA} --output ${newfile}_derep.fasta --sizeout

	docker stop itsx_run_$TIMESTAMP
	docker rm itsx_run_$TIMESTAMP

elif [ $GENE = "ITS-PLANTS-NCBI" ];
then
	echo  "Creating an ITSx Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $FULL_PATH_RAW:/rawdata/ --name itsx_run_$TIMESTAMP metashot/itsx:1.1.2-1

	echo  "Running the ITSx Container: "
	docker exec -u $(id -u) -i itsx_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		ITSx -i /rawdata/'${FILE_NAME_RAW}' -o '${newfile}_itsx' -t t,b \
		--cpu '$THREADS' --graphical F; chmod -R 777 /output/'$OUTPUT';'


	cat *_itsx.ITS1.fasta *_itsx.ITS2.fasta > ${newfile}_itsx.fasta
	chmod 777 ${newfile}_itsx.fasta

	echo "Running the VSEARCH Container - --derep_fulllength: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --derep_fulllength '${newfile}_itsx.fasta' --output '${newfile}_derep.fasta' --sizeout; \
		chmod -R 777 /output/'$OUTPUT';'
	#vsearch --derep_fulllength ../${RAWDATA} --output ${newfile}_derep.fasta --sizeout

	docker stop itsx_run_$TIMESTAMP
	docker rm itsx_run_$TIMESTAMP
elif [ $ITS = "its" ];
then
	echo  "Creating an ITSx Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $FULL_PATH_RAW:/rawdata/ --name itsx_run_$TIMESTAMP metashot/itsx:1.1.2-1

	echo  "Running the ITSx Container: "
	docker exec -u $(id -u) -i itsx_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		ITSx -i /rawdata/'${FILE_NAME_RAW}' -o '${newfile}_itsx' -t t,b,f \
		--cpu '$THREADS' --graphical F; chmod -R 777 /output/'$OUTPUT';'


	cat *_itsx.ITS1.fasta *_itsx.ITS2.fasta > ${newfile}_itsx.fasta
	chmod 777 ${newfile}_itsx.fasta

	echo "Running the VSEARCH Container - --derep_fulllength: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --derep_fulllength '${newfile}_itsx.fasta' --output '${newfile}_derep.fasta' --sizeout; \
		chmod -R 777 /output/'$OUTPUT';'
	#vsearch --derep_fulllength ../${RAWDATA} --output ${newfile}_derep.fasta --sizeout

	docker stop itsx_run_$TIMESTAMP
	docker rm itsx_run_$TIMESTAMP
else
	echo "Running the VSEARCH Container - --derep_fulllength: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --derep_fulllength /rawdata/'${FILE_NAME_RAW}' --output '${newfile}_derep.fasta' --sizeout; \
		chmod -R 777 /output/'$OUTPUT';'
	#vsearch --derep_fulllength ../${RAWDATA} --output ${newfile}_derep.fasta --sizeout
fi

#Abundance sort and discard singletons <<<USING VSEARCH>>
echo "Running the VSEARCH Container - --sortbysize: "
docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --sortbysize '${newfile}_derep.fasta' --output '${newfile}_sorted.fasta' --minsize 2; \
	chmod 777 '${newfile}_sorted.fasta';'
#vsearch --sortbysize ${newfile}_derep.fasta --output ${newfile}_sorted.fasta --minsize 2

# Shortening reads in the ITS extracted FASTA file <<<USING VSEARCH>>>
if [ $LENGTH != "0" ];
then
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --fastx_filter  '${newfile}_sorted.fasta' --fastq_trunclen '$LENGTH' \
	--fastaout '${newfile}_trimmed.fasta'; chmod 777 '${newfile}_trimmed.fasta';'
	#vsearch --fastx_filter  ${newfile}_sorted.fasta --fastq_trunclen $LENGTH --fastaout ${newfile}_trimmed.fasta
else
	cp ${newfile}_sorted.fasta ${newfile}_trimmed.fasta
fi

if [ $APPROACH = "otu" ];
then
	#OTU clustering using UPARSE method <<<USING VSEARCH>>>
	echo "Running the VSEARCH Container - --cluster_size: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --cluster_size '${newfile}_trimmed.fasta' --consout '${newfile}_otus1.fasta' \
		--id '$SIMILARITY' --threads '$THREADS' ; chmod 777 '${newfile}_otus1.fasta';'
	#vsearch --cluster_size ${newfile}_trimmed.fasta --consout ${newfile}_otus1.fasta --id $SIMILARITY
elif [ $APPROACH = "asv" ];
then
	#Discarding reads with more than 1 Ns, since swarm cannot handle it

	echo "Running the VSEARCH Container - --fastx_filter: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		vsearch --fastx_filter '${newfile}_trimmed.fasta' --fastq_maxns 0 --fastaout '${newfile}_trimmed_noN.fasta' ; \
		chmod 777  '${newfile}_trimmed_noN.fasta';'

	echo "Creating a SWARM Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name swarm_run_$TIMESTAMP itvdsbioinfo/pimba_swarm:v3.1.0

	#Finding ASVs using SWARM method
	echo "Running the SWARM Container: "
	docker exec -u $(id -u) -i swarm_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
		swarm -d 0 -z -t '$THREADS' -w '${newfile}_trimmed_noN_derep.fasta' -o /dev/null '${newfile}_trimmed_noN.fasta'; \
		swarm -f -t '$THREADS' -l swarm.err -o swarm.log -w '${newfile}_otus1.fasta' -u uclust_format.file -z \
		-d 1 '${newfile}_trimmed_noN_derep.fasta'; chmod 777 '${newfile}_otus1.fasta' '${newfile}_trimmed_noN_derep.fasta';'

	SIMILARITY_INT=97

	docker stop swarm_run_$TIMESTAMP
	docker rm swarm_run_$TIMESTAMP

fi

#Chimera filtering using de novo strategy <<<USING VSEARCH>>>
echo "Running the VSEARCH Container - --uchime_denovo: "
docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --uchime_denovo '${newfile}_otus1.fasta' --fasta_width 0 -uchimeout '${newfile}_otus_nochim' \
	--chimeras '${newfile}_chim.fasta' --nonchimeras '${newfile}_noChim.fasta'; \
	chmod 777 '${newfile}_otus_nochim' '${newfile}_chim.fasta' '${newfile}_noChim.fasta';'
#vsearch --uchime_denovo ${newfile}_otus1.fasta --fasta_width 0 -uchimeout ${newfile}_otus_nochim --chimeras ${newfile}_chim.fasta --nonchimeras ${newfile}_noChim.fasta

# Fasta Formatter <<<FASTX TOOLKIT SCRIPT>>>
echo "Creating and running a fasxttoolkit Container: "
docker run -u $(id -u) -i -v $CURRENT_PATH:/output/ itvdsbioinfo/pimba_fastxtoolkit:v0.0.14 fasta_formatter -i /output/${OUTPUT}/${newfile}_noChim.fasta -o /output/${OUTPUT}/${newfile}_formated_otus2.fasta
#fasta_formatter -i ${newfile}_noChim.fasta -o ${newfile}_formated_otus2.fasta

#Renamer <<<BMP SCRIPT>>>
echo "Creating a Perl Container: "
docker run -id -v $CURRENT_PATH:/output/ --name perl_run_$TIMESTAMP itvdsbioinfo/pimba_perl:v5

echo "Running the Perl Container: "
docker exec -u $(id -u) -i perl_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	perl /data/bmp-otuName.pl -i '${newfile}_formated_otus2.fasta' -o '${newfile}_otus.fasta'; \
	chmod 777 '${newfile}_formated_otus2.fasta' '${newfile}_otus.fasta';'
#perl ${BMP_PATH}/bmp-otuName.pl -i ${newfile}_formated_otus2.fasta -o ${newfile}_otus.fasta

if [ $APPROACH = "asv" ];
then
	cp ${newfile}_otus.fasta ${newfile}_asv.fasta
fi

#Map reads back to OTU database <<<VSEARCH script>>>
echo "Running the VSEARCH Container - --usearch_global: "
docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --usearch_global /rawdata/'${FILE_NAME_RAW}' --db '${newfile}'_otus.fasta --strand both \
	--id '$SIMILARITY' --uc '${newfile}'_map.uc --threads '$THREADS' ; \
	chmod 777 '${newfile}'_map.uc;'
#vsearch --usearch_global ../${RAWDATA} --db ${newfile}_otus.fasta --strand both --id $SIMILARITY --uc ${newfile}_map.uc

#Convert UC to otu-table.txt <<< BMP SCRIPT>>>
echo "Creating a QiimePipe Container: "
docker run -id -v $CURRENT_PATH:/output/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

echo "Running the QiimePipe Container - uc2otutab: "
docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	python3.6 /qiimepipe/uc2otutab.py '${newfile}'_map.uc > '${newfile}'_otu_table.txt;\
	chmod 777 '${newfile}'_otu_table.txt'
#python ${BMP_PATH}/uc2otutab.py ${newfile}_map.uc > ${newfile}_otu_table.txt

if [ $APPROACH = "asv" ];
then
	cp ${newfile}_otu_table.txt ${newfile}_asv_table.txt
fi

#getting the number of samples through the otu/asv.table
NUMSAMPLES=$(awk '{print split($0,a,"\t"); exit}' ${newfile}_otu_table.txt)

##########################################
#Run LULU to generate a curated otu_table if number of samples is greater than 1#
if [ $LULU = "lulu" ];
then
	export BLASTDB=$NCBI_DB_EXP
	#Run makeblastdb to create a blast database sing the otus fasta file
	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - makeblastdb: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; mkdir lulu_output; cd lulu_output/; \
		makeblastdb -in ../'${newfile}'_otus.fasta -out '${newfile}'_otus.fasta -parse_seqids -dbtype nucl ; \
		chmod -R 777 ../lulu_output'
	#makeblastdb -in ${newfile}_otus.fasta -parse_seqids -dbtype nucl

	#Make a blast with the OTUs against the same OTUs database created in the previous step
	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/lulu_output/; \
		blastn -db '${newfile}'_otus.fasta -outfmt "6 qseqid sseqid pident" -out match_list.txt \
		-num_threads '$THREADS' -qcov_hsp_perc 80 -perc_identity 84 -query ../'${newfile}'_otus.fasta; \
		chmod 777 match_list.txt'
	#blastn -db ${newfile}_otus.fasta -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 80 -perc_identity 84 -query ${newfile}_otus.fasta

	#Run LULU with the otu_table and the match_list file created in the previous step
	echo "Creating an R Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name rdocker_run_$TIMESTAMP itvdsbioinfo/pimba_r:latest bash

	echo "Running the R Container - lulu: "
	docker exec -u $(id -u) -i rdocker_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/lulu_output/; \
		Rscript /data/file.R ../'${newfile}'_otu_table.txt match_list.txt '${newfile}'_otu_table_lulu.txt; \
		chmod 777 '${newfile}'_otu_table_lulu.txt lulu.log*'
	#Rscript ${SCRIPT_PATH}/file.R ${newfile}_otu_table.txt match_list.txt ${newfile}_otu_table_lulu.txt

	#Remove "" from the lulu new otu table
	sed -i -e 's/"X//g' lulu_output/${newfile}_otu_table_lulu.txt
	sed -i -e 's/"//g' lulu_output/${newfile}_otu_table_lulu.txt

	#Add the "OTUId" column name
	sed -i '1s/^/OTUId\t/' lulu_output/${newfile}_otu_table_lulu.txt

	#Rename the otu_table files
	mv ${newfile}_otu_table.txt ${newfile}_otu_table_old.txt
	cp lulu_output/${newfile}_otu_table_lulu.txt ${newfile}_otu_table.txt

	docker stop rdocker_run_$TIMESTAMP
	docker rm rdocker_run_$TIMESTAMP

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
fi



if [ $GENE = "16S-SILVA" ];
then
	echo "Creating a Qiime Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $SILVA_DB_16S:/database/ --name qiime_run_$TIMESTAMP itvdsbioinfo/pimba_qiime:latest

	#Assign taxonomy to OTUS using blast method on QIIME
	echo "Running the Qiime Container - assign_taxonomy.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	assign_taxonomy.py -i '${newfile}'_otus.fasta -o output -t /database/taxonomy/16S_only/'${SIMILARITY_INT}'/taxonomy_7_levels.txt \
	-r /database/rep_set/rep_set_16S_only/'${SIMILARITY_INT}'/silva_132_'${SIMILARITY_INT}'_16S.fna --similarity='$SIMILARITY_ASSIGN'; \
	chmod -R 777 output'
	#assign_taxonomy.py -i ${newfile}_otus.fasta -o output -t ${SILVA_DB_16S}/taxonomy/16S_only/${SIMILARITY_INT}/taxonomy_7_levels.txt -r ${SILVA_DB_16S}/rep_set/rep_set_16S_only/${SIMILARITY_INT}/silva_132_${SIMILARITY_INT}_16S.fna --similarity=$SIMILARITY_ASSIGN
	
	#Align sequences on QIIME, using greengenes reference sequences (use the file “otus.fa” from UPARSE as input file)
	echo "Running the Qiime Container - align_seqs.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	align_seqs.py -i '${newfile}'_otus.fasta -o rep_set_align -t /database/rep_set_aligned/'${SIMILARITY_INT}'/'${SIMILARITY_INT}'_alignment.fna; \
	chmod -R 777 rep_set_align'
	#align_seqs.py -i ${newfile}_otus.fasta -o rep_set_align -t ${SILVA_DB_16S}/rep_set_aligned/${SIMILARITY_INT}/${SIMILARITY_INT}_alignment.fna

	#Filter alignments on QIIME
	echo "Running the Qiime Container - filter_alignment.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	filter_alignment.py -i rep_set_align/'${newfile}'_otus_aligned.fasta -o filtered_alignment; \
	chmod -R 777 filtered_alignment'
	#filter_alignment.py -i rep_set_align/${newfile}_otus_aligned.fasta -o filtered_alignment

	#Make the reference tree on QIIME
	echo "Running the Qiime Container - make_phylogeny.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	make_phylogeny.py -i filtered_alignment/'${newfile}'_otus_aligned_pfiltered.fasta -o rep_set.tre; \
	chmod -R 777 rep_set.tre'
	#make_phylogeny.py -i filtered_alignment/${newfile}_otus_aligned_pfiltered.fasta -o rep_set.tre

	mkdir diversity_by_sample
	cd diversity_by_sample

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime

	echo "Running the QiimePipe Container - createAbundanceFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createAbundanceFile.py ../output/'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_otu_table.txt;\
	chmod -R 777 ../diversity_by_sample'
	#python ${SCRIPT_PATH}/createAbundanceFile.py ../output/${newfile}_otus_tax_assignments.txt ../${newfile}_otu_table.txt
	cd ..

	docker stop qiime_run_$TIMESTAMP
	docker rm qiime_run_$TIMESTAMP


elif [ $GENE = "16S-GREENGENES" ];
then

	echo "Creating a Qiime Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $GG_DB_16S:/database/ --name qiime_run_$TIMESTAMP itvdsbioinfo/pimba_qiime:latest

	#Assign taxonomy to OTUS using blast method on QIIME
	echo "Running the Qiime Container - assign_taxonomy.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	assign_taxonomy.py -i '${newfile}'_otus.fasta -o output -t /database/taxonomy/${SIMILARITY_INT}_otu_taxonomy.txt \
	-r /database/rep_set/${SIMILARITY_INT}_otus.fasta --similarity='$SIMILARITY_ASSIGN'; \
	chmod -R 777 output'
	#assign_taxonomy.py -i ${newfile}_otus.fasta -o output -t ${GG_DB_16S}/taxonomy/${SIMILARITY_INT}_otu_taxonomy.txt -r ${$GG_DB_16S}/rep_set/${SIMILARITY_INT}_otus.fasta --similarity=$SIMILARITY_ASSIGN

	#Align sequences on QIIME, using greengenes reference sequences (use the file “otus.fa” from UPARSE as input file)
	echo "Running the Qiime Container - align_seqs.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	align_seqs.py -i '${newfile}'_otus.fasta -o rep_set_align -t /database/rep_set_aligned/'${SIMILARITY_INT}_otus.fasta'; \
	chmod -R 777 rep_set_align'
	#align_seqs.py -i ${newfile}_otus.fasta -o rep_set_align -t ${GG_DB_16S}/rep_set_aligned/${SIMILARITY_INT}_otus.fasta

	#Filter alignments on QIIME
	echo "Running the Qiime Container - filter_alignment.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	filter_alignment.py -i rep_set_align/'${newfile}'_otus_aligned.fasta -o filtered_alignment; \
	chmod -R 777 filtered_alignment'
	#filter_alignment.py -i rep_set_align/${newfile}_otus_aligned.fasta -o filtered_alignment

	#Make the reference tree on QIIME
	echo "Running the Qiime Container - make_phylogeny.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	make_phylogeny.py -i filtered_alignment/'${newfile}'_otus_aligned_pfiltered.fasta -o rep_set.tre; \
	chmod -R 777 rep_set.tre'
	#make_phylogeny.py -i filtered_alignment/${newfile}_otus_aligned_pfiltered.fasta -o rep_set.tre

	mkdir diversity_by_sample
	cd diversity_by_sample

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - createAbundanceFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createAbundanceFile.py ../output/'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_otu_table.txt;\
	chmod -R 777 ../diversity_by_sample'
	#python ${SCRIPT_PATH}/createAbundanceFile.py ../output/${newfile}_otus_tax_assignments.txt ../${newfile}_otu_table.txt
	cd ..

	docker stop qiime_run_$TIMESTAMP
	docker rm qiime_run_$TIMESTAMP

elif [ $GENE = "16S-RDP" ];
then

	echo "Creating a Qiime Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $RDP_DB_16S:/database/ --name qiime_run_$TIMESTAMP itvdsbioinfo/pimba_qiime:latest


	#Assign taxonomy to OTUS using blast method on QIIME
	echo "Running the Qiime Container - assign_taxonomy.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	assign_taxonomy.py -i '${newfile}'_otus.fasta -o output -t /database/*.t* \
	-r /database/rep_set/*.fa* --similarity='$SIMILARITY_ASSIGN'; \
	chmod -R 777 output'
	#assign_taxonomy.py -i ${newfile}_otus.fasta -o output -t ${RDP_DB_16S}/trainset16_022016.rdp.tax -r ${RDP_DB_16S}/trainset16_022016.rdp.fasta --similarity=$SIMILARITY_ASSIGN

	#Align sequences on QIIME, using RDP reference sequences (use the file “otus.fa” from UPARSE as input file)
	echo "Running the Qiime Container - align_seqs.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	align_seqs.py -i '${newfile}'_otus.fasta -o rep_set_align -t /database/*align*; \
	chmod -R 777 rep_set_align'
	#align_seqs.py -i ${newfile}_otus.fasta -o rep_set_align -t ${RDP_DB_16S}/trainset16_022016.rdp.align.fasta

	#Filter alignments on QIIME
	echo "Running the Qiime Container - filter_alignment.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	filter_alignment.py -i rep_set_align/'${newfile}'_otus_aligned.fasta -o filtered_alignment; \
	chmod -R 777 filtered_alignment'
	#filter_alignment.py -i rep_set_align/${newfile}_otus_aligned.fasta -o filtered_alignment

	#Make the reference tree on QIIME
	echo "Running the Qiime Container - make_phylogeny.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	make_phylogeny.py -i filtered_alignment/'${newfile}'_otus_aligned_pfiltered.fasta -o rep_set.tre; \
	chmod -R 777 rep_set.tre'
	#make_phylogeny.py -i filtered_alignment/${newfile}_otus_aligned_pfiltered.fasta -o rep_set.tre

	mkdir diversity_by_sample
	cd diversity_by_sample

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiimeecho "Running the QiimePipe Container - createAbundanceFile.py: "
	echo "Running the QiimePipe Container - createAbundanceFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createAbundanceFile.py ../output/'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_otu_table.txt;\
	chmod -R 777 ../diversity_by_sample'
	#python ${SCRIPT_PATH}/createAbundanceFile.py ../output/${newfile}_otus_tax_assignments.txt ../${newfile}_otu_table.txt
	cd ..

	docker stop qiime_run_$TIMESTAMP
	docker rm qiime_run_$TIMESTAMP

elif [ $GENE = "16S-NCBI" ];
then
	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP

	#Assign taxonomy to OTUS using blast. The blast database is needed.
	#export BLASTDB=$NCBI_DB_EXP

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log

	mkdir diversity_by_sample
	cd diversity_by_sample

	#Convert UC to otu-table.txt <<< BMP SCRIPT>>>
	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $TAXDUMP:/taxdump/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2
	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - createTaxonTable_singleFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile.py ../'${newfile}'_blast.log \
	../'${newfile}'_otu_table.txt /taxdump/rankedlineage.dmp; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile.py ../${newfile}_blast.log ../${newfile}_otu_table.txt
	cd ../

	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt output/
	

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP

elif [ $GENE = "ITS-FUNGI-NCBI" ];
then

	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP

	#Filter out contaminants basing on ncbi/nt blast
	export BLASTDB=$NCBI_DB_EXP

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast_ncbi.log; chmod 777 '${newfile}'_blast_ncbi.log'

	#blastn -query ${newfile}_otus.fasta -task megablast -db  $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast_ncbi.log

	mkdir diversity_by_sample_ncbi
	cd diversity_by_sample_ncbi

	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $TAXDUMP:/taxdump/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - create_otuTaxAssignment.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample_ncbi; \
	python3.6 /qiimepipe/create_otuTaxAssignment.py ../'${newfile}'_blast_ncbi.log \
	../'${newfile}'_otu_table.txt ncbi_otus_tax_assignments.txt; chmod -R 777 ../diversity_by_sample_ncbi'
	#python ${SCRIPT_PATH}/create_otuTaxAssignment.py ../${newfile}_blast_ncbi.log ../${newfile}_otu_table.txt ncbi_otus_tax_assignments.txt

	cd ..

	#Obtaining only the OTUs classified as Fungi
	echo "Running the QiimePipe Container - filterOTUs.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	python3.6 /qiimepipe/filterOTUs.py '${newfile}'_otus.fasta diversity_by_sample_ncbi/ncbi_otus_tax_assignments.txt; \
	chmod 777 k__Fungi.* *filtered.fasta *contigIDs.txt'
	#python ${SCRIPT_PATH}/filterOTUs.py ${newfile}_otus.fasta diversity_by_sample_ncbi/ncbi_otus_tax_assignments.txt

	#Assign taxonomy to OTUS using blast. The blast database is needed.
	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query k__Fungi.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast_fungi.log; chmod 777 '${newfile}'_blast_fungi.log'
	#blastn -query k__Fungi.fasta -task megablast -db $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -parse_deflines -evalue $EVALUE -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast_fungi.log

	#Map reads back to OTU database <<<VSEARCH script>>>
	echo "Running the VSEARCH Container - --usearch_global: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --usearch_global /rawdata/'${FILE_NAME_RAW}' --db k__Fungi.fasta --strand both \
	--id '$SIMILARITY' --uc '${newfile}'_map_fungi.uc; \
	chmod 777 '${newfile}'_map_fungi.uc;'
	#vsearch  --usearch_global ../${RAWDATA} --db k__Fungi.fasta --strand both --id $SIMILARITY --uc ${newfile}_map_fungi.uc

	#Convert UC to otu-table.txt <<< BMP SCRIPT>>>
	echo "Running the QiimePipe Container - uc2otutab: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	python3.6 /qiimepipe/uc2otutab.py '${newfile}'_map_fungi.uc > '${newfile}'_otu_table_fungi.txt;\
	chmod 777 '${newfile}'_otu_table_fungi.txt'
	#python ${BMP_PATH}/uc2otutab.py ${newfile}_map_fungi.uc > ${newfile}_otu_table_fungi.txt

	mkdir diversity_by_sample
	cd diversity_by_sample
	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime

	echo "Running the QiimePipe Container - createTaxonTable_singleFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile.py ../'${newfile}'_blast_fungi.log \
	../'${newfile}'_otu_table_fungi.txt /taxdump/rankedlineage.dmp; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile.py ../${newfile}_blast_fungi.log ../${newfile}_otu_table_fungi.txt
	cd ../
	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt output/

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP


elif [ $GENE = "ITS-FUNGI-UNITE" ];
then



	echo "Creating a Qiime Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $ITS_UNITE_DB:/database/ --name qiime_run_$TIMESTAMP itvdsbioinfo/pimba_qiime:latest

	#Assign taxonomy to OTUS using blast method on QIIME. Use the file .otus.fa. from UPARSE as input file and UNITE as reference database (Download UNITE database HERE)
	echo "Running the Qiime Container - assign_taxonomy.py: "
	docker exec -u $(id -u) -i qiime_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	assign_taxonomy.py -i '${newfile}'_otus.fasta -o output -t /database/*'${SIMILARITY_INT}'*.txt \
	-r /database/*'${SIMILARITY_INT}'*.fasta --similarity='$SIMILARITY_ASSIGN'; \
	chmod -R 777 output'
	#assign_taxonomy.py -i ${newfile}_otus.fasta -o output -r ${ITS_UNITE_DB}/sh_refs_qiime_ver8_${SIMILARITY_INT}_s_04.02.2020.fasta -t  ${ITS_UNITE_DB}/sh_taxonomy_qiime_ver8_${SIMILARITY_INT}_s_04.02.2020.txt --similarity=$SIMILARITY_ASSIGN
	mkdir diversity_by_sample
	cd diversity_by_sample

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiimeecho "Running the QiimePipe Container - createAbundanceFile.py: "
	echo "Running the QiimePipe Container - createAbundanceFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createAbundanceFile.py ../output/'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_otu_table.txt;\
	chmod -R 777 ../diversity_by_sample'
	#python ${SCRIPT_PATH}/createAbundanceFile.py ../output/${newfile}_otus_tax_assignments.txt ../${newfile}_otu_table.txt
	cd ..

	docker stop qiime_run_$TIMESTAMP
	docker rm qiime_run_$TIMESTAMP

elif [ $GENE = "ITS-PLANTS-NCBI" ];
then

	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP
	

	#Filter out contaminants basing on ncbi/nt blast
	#export BLASTDB=$NCBI_DB_EXP

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT';\
		blastn -query '${newfile}'_otus.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast_ncbi.log; chmod 777 '${newfile}'_blast_ncbi.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db  $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast_ncbi.log

	mkdir diversity_by_sample_ncbi
	cd diversity_by_sample_ncbi

	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $TAXDUMP:/taxdump/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - create_otuTaxAssignment.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample_ncbi; \
	python3.6 /qiimepipe/create_otuTaxAssignment.py ../'${newfile}'_blast_ncbi.log \
	../'${newfile}'_otu_table.txt ncbi_otus_tax_assignments.txt; chmod -R 777 ../diversity_by_sample_ncbi'
	#python ${SCRIPT_PATH}/create_otuTaxAssignment.py ../${newfile}_blast_ncbi.log ../${newfile}_otu_table.txt ncbi_otus_tax_assignments.txt

	cd ..

	#Filterin out the OTUs classified as Fungi
	echo "Running the QiimePipe Container - filterOTUs.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	python3.6 /qiimepipe/filterOTUs.py '${newfile}'_otus.fasta diversity_by_sample_ncbi/ncbi_otus_tax_assignments.txt; \
	chmod 777 k__Fungi.* '${newfile}'_otus_filtered.fasta'
	#python ${SCRIPT_PATH}/filterOTUs.py ${newfile}_otus.fasta diversity_by_sample_ncbi/ncbi_otus_tax_assignments.txt

	#Assign taxonomy to OTUS using blast. The blast database is needed.
	#blastn -query ${newfile}_otus_filtered.fasta -task megablast -db $DATABASE -max_target_seqs 1 -parse_deflines -num_threads 24 -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log
	#blastn -query ${newfile}_otus.fasta -task megablast -db $DATABASE -max_target_seqs 1 -parse_deflines -num_threads 24 -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log
	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus_filtered.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast_plants.log; chmod 777 '${newfile}'_blast_plants.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log

	#Map reads back to OTU database <<<VSEARCH script>>>
	echo "Running the VSEARCH Container - --usearch_global: "
	docker exec -u $(id -u) -i vsearch_run_$TIMESTAMP  /bin/bash -c 'cd /output/'$OUTPUT'; \
	vsearch --usearch_global /rawdata/'${FILE_NAME_RAW}' --db '${newfile}'_otus_filtered.fasta --strand both \
	--id '$SIMILARITY' --uc '${newfile}'_map_plants.uc; \
	chmod 777 '${newfile}'_map_plants.uc;'
	#vsearch  --usearch_global ../${RAWDATA} --db k__Fungi.fasta --strand both --id $SIMILARITY --uc ${newfile}_map_fungi.uc

	#Convert UC to otu-table.txt <<< BMP SCRIPT>>>
	echo "Running the QiimePipe Container - uc2otutab: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
	python3.6 /qiimepipe/uc2otutab.py '${newfile}'_map_plants.uc > '${newfile}'_otu_table_plants.txt;\
	chmod 777 '${newfile}'_otu_table_plants.txt'
	#python ${BMP_PATH}/uc2otutab.py ${newfile}_map_fungi.uc > ${newfile}_otu_table_fungi.txt


	mkdir diversity_by_sample
	cd diversity_by_sample

	echo "Running the QiimePipe Container - createTaxonTable_singleFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile.py ../'${newfile}'_blast_plants.log \
	../'${newfile}'_otu_table_plants.txt /taxdump/rankedlineage.dmp; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile.py ../${newfile}_blast.log ../${newfile}_otu_table.txt
	cd ../
	mkdir output
	
	mv *_otus_tax_assignments.txt ${newfile}_otus_tax_assignments.txt
	mv ${newfile}_otus_tax_assignments.txt output/

	chmod -R 777 output

	

	cp ${newfile}_otus_filtered.fasta ${newfile}_otus_plants.fasta

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
	

elif [ $GENE = "COI-NCBI" ];
then

	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP
	

	#Assign taxonomy to OTUS using blast. The blast database is needed.
	#export BLASTDB=$NCBI_DB_EXP

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $BLASTDB:/blastdb/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	# echo "Running the BLAST Container - blastn: "
	# docker exec -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; export BLASTDB=/blastdb/; \
	# 	blastn -query '${newfile}'_otus.fasta -task megablast -db /blastdb/nt -perc_identity \
	# 	'$SIMILARITY_INT' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
	# 	-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines -num_threads \
	# 	'$THREADS' -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
	# 	'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus.fasta -task megablast -db nt -remote -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log
	
	mkdir diversity_by_sample
	cd diversity_by_sample

	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $TAXDUMP:/taxdump/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - createTaxonTable_singleFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile.py ../'${newfile}'_blast.log \
	../'${newfile}'_otu_table.txt /taxdump/rankedlineage.dmp; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile.py ../${newfile}_blast.log ../${newfile}_otu_table.txt
	cd ../
	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt output/

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
	

elif [ $GENE = "COI-BOLD" ];
then

	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP
	


	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $COI_BOLD_DB:/coibold/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest


	#Assign taxonomy to OTUS using blast. The blast database is needed.
	#assign_taxonomy.py -i ${newfile}_otus.fasta -o output -t ${COI_BOLD_DB}_tax.txt -r ${COI_BOLD_DB}.fasta --similarity=$SIMILARITY_ASSIGN
	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus.fasta -task megablast -db /coibold/*.fasta -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines -num_threads \
		'$THREADS' -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db ${COI_BOLD_DB}.fasta -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log

	mkdir diversity_by_sample
	cd diversity_by_sample

	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $COI_BOLD_DB:/coibold/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - createTaxonTable_singleFile_flex.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile_flex.py ../'${newfile}'_blast.log \
	../'${newfile}'_otu_table.txt /coibold/*_tax.txt; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_taxon_red_flagged.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile_flex.py ../${newfile}_blast.log ../${newfile}_otu_table.txt ${COI_BOLD_DB}_tax.txt
	cd ..
	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt ${newfile}_taxon_red_flagged.txt output/
	
	
	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile_flex.py ${newfile}_blast.log ${newfile}_otu_table.txt
	#mkdir output
	#mv ${newfile}_otus_tax_assignments.txt output/	

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
	
elif [ $GENE = "ALL-NCBI" ];
then
	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP

	#Assign taxonomy to OTUS using blast. The blast database is needed.
	export BLASTDB=$NCBI_DB_EXP

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $BLASTDB:/blastdb/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; export BLASTDB=/blastdb/;\
		blastn -query '${newfile}'_otus.fasta -task megablast -db /blastdb/nt -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines -num_threads '$THREADS' \
		-outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db $NCBI_DB -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log

	mkdir diversity_by_sample
	cd diversity_by_sample

	#Convert UC to otu-table.txt <<< BMP SCRIPT>>>
	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $TAXDUMP:/taxdump/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2
	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Running the QiimePipe Container - createTaxonTable_singleFile.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile.py ../'${newfile}'_blast.log \
	../'${newfile}'_otu_table.txt /taxdump/rankedlineage.dmp; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt'
	#python ${SCRIPT_PATH}/createTaxonTable_singleFile.py ../${newfile}_blast.log ../${newfile}_otu_table.txt
	cd ../

	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt output/
	

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
else
	docker stop qiimepipe_run_$TIMESTAMP
	docker rm qiimepipe_run_$TIMESTAMP
	

	echo "Creating a BLAST Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $GENE:/gene/ --name blast_run_$TIMESTAMP itvdsbioinfo/pimba_blast:latest

	echo "Running the BLAST Container - blastn: "
	docker exec -u $(id -u) -i blast_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
		blastn -query '${newfile}'_otus.fasta -task megablast -db /gene/*.fasta -perc_identity \
		'$SIMILARITY_INT_ASG' -qcov_hsp_perc '$COVERAGE_INT' -max_hsps '$HITS_PER_SUBJECT' \
		-max_target_seqs '$HITS_PER_SUBJECT' -evalue '$EVALUE' -parse_deflines -num_threads \
		'$THREADS' -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > \
		'${newfile}'_blast.log; chmod 777 '${newfile}'_blast.log'
	#blastn -query ${newfile}_otus.fasta -task megablast -db $GENE -perc_identity $SIMILARITY_INT -qcov_hsp_perc $COVERAGE_INT -max_hsps $HITS_PER_SUBJECT -max_target_seqs $HITS_PER_SUBJECT -evalue $EVALUE -parse_deflines -num_threads $THREADS -outfmt "6 qseqid sscinames sseqid staxids stitle pident qcovs evalue" > ${newfile}_blast.log

	mkdir diversity_by_sample
	cd diversity_by_sample
	#Generate individual diversity information for each sample in the data and convert the blast file to otu_tax_assignment file from Qiime
	echo "Creating a QiimePipe Container: "
	docker run -id -v $CURRENT_PATH:/output/ -v $GENE:/gene/ --name qiimepipe_run_$TIMESTAMP itvdsbioinfo/pimba_qiimepipe:v2

	echo "Running the QiimePipe Container - createTaxonTable_singleFile_flex.py: "
	docker exec -u $(id -u) -i qiimepipe_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'/diversity_by_sample; \
	python3.6 /qiimepipe/createTaxonTable_singleFile_flex.py ../'${newfile}'_blast.log \
	../'${newfile}'_otu_table.txt /gene/*_tax.txt; \
	chmod -R 777 ../diversity_by_sample; chmod 777 ../'${newfile}'_otus_tax_assignments.txt ../'${newfile}'_taxon_red_flagged.txt'

	#python ${SCRIPT_PATH}/createTaxonTable_singleFile_flex.py ../${newfile}_blast.log ../${newfile}_otu_table.txt <colocar tax>
	cd ../
	mkdir output
	chmod -R 777 output
	mv ${newfile}_otus_tax_assignments.txt ${newfile}_taxon_red_flagged.txt output/

	docker stop blast_run_$TIMESTAMP
	docker rm blast_run_$TIMESTAMP
	
fi


#Convert otu_table.txt to otu-table.biom, used by QIIME <<< BIOM SCRIPT>>>
echo "Creating a Biom-Format Container: "
docker run -id -v $CURRENT_PATH:/output/ --name biomformat_run_$TIMESTAMP itvdsbioinfo/pimba_biom:v2.1.10

echo "Running the Biom-Format Container - convert: "
docker exec -u $(id -u) -i biomformat_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
biom convert -i '${newfile}'_otu_table.txt -o '${newfile}'_otu_table.biom \
--table-type="OTU table" --to-json; chmod 777 '${newfile}'_otu_table.biom;'
#biom convert -i ${newfile}_otu_table.txt -o ${newfile}_otu_table.biom --table-type="OTU table" --to-json

#Add metadata (taxonomy) to OTU table
echo "Running the Biom-Format Container - add-metadata: "
docker exec -u $(id -u) -i biomformat_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
biom add-metadata -i '${newfile}'_otu_table.biom -o '${newfile}'_otu_table_tax.biom \
--observation-metadata-fp output/'${newfile}'_otus_tax_assignments.txt --observation-header OTUID,taxonomy,confidence \
--sc-separated taxonomy --float-fields confidence; chmod 777 '${newfile}'_otu_table_tax.biom;'
#biom add-metadata -i ${newfile}_otu_table.biom -o ${newfile}_otu_table_tax.biom --observation-metadata-fp output/${newfile}_otus_tax_assignments.txt --observation-header OTUID,taxonomy,confidence --sc-separated taxonomy --float-fields confidence

# Check OTU Table  on QIIME.
echo "Running the Biom-Format Container - summarize-table: "
docker exec -u $(id -u) -i biomformat_run_$TIMESTAMP /bin/bash -c 'cd /output/'$OUTPUT'; \
biom summarize-table -i '${newfile}'_otu_table_tax.biom -o '${newfile}'_biom_table; \
chmod 777 '${newfile}'_biom_table;'
#biom summarize-table -i ${newfile}_otu_table_tax.biom -o ${newfile}_biom_table

#conda deactivate

echo "Stopping Containeres: "
docker stop vsearch_run_$TIMESTAMP
docker stop perl_run_$TIMESTAMP
docker stop qiimepipe_run_$TIMESTAMP
docker stop biomformat_run_$TIMESTAMP


echo "Removing Containeres: "
docker rm vsearch_run_$TIMESTAMP
docker rm perl_run_$TIMESTAMP
docker rm qiimepipe_run_$TIMESTAMP
docker rm biomformat_run_$TIMESTAMP

#Get the OTU amount from the sample with the minimum value
#min="$(grep "Min: " ${newfile}_biom_table)"
#arrmin=(${min//;/ })
#min="$(echo "${arrmin[1]}")"
#min=${min%.*}

#Run diversity analyses on QIIME by applying non-phylogenetic metrics
#core_diversity_analyses.py -i ${newfile}_otu_table_tax.biom -m ../mapping_file.txt -e $min -o core_output --nonphylogenetic_diversity